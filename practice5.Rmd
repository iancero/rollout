---
title: "R Notebook"
---

# 2024-08-02

Try to implement **coxph** (without! random effects)

Consider also "frailty models" with random effects terms, run an example (`?frailty:random effect terms` has examples that you can run). Frailty model is basically coxph + random effects (e.g., for site)


# Assignment for current week

Put together annotated code for Hendricks' simulations in paper 1. Should result in a figure that that compares parallel designs to a step wedge design with increasing levels of variance (y axis should be power)

send figure as tiff and pdf

Type 1 error, num replications, random effects we can pick (what there variances are), what the model itself if for analyzing data


# Put together a parallel design vs rollout design example for hendricks, like in his email


# NOTE: for clarity, do fixed effects before random effects

# TODO (2024-06-14):

Start with generic version of specifying parameters
  - Model parameters
  - Population generating parameters
      - I want to change not just effect sizes, but also ICCs
  - Output parameters (what kinds of things do you want to see)
  - Replication-based parameters (how many?)

Next steps are entering parameters:
  - Model characteristics, what kind of model are you using (e.g., continuous outcome vs glmer)
  - Allow user to specify names of columns and units
  - Other parameters based on the simulations you want to do, how many replicates are you looking for
  - What are the **collections** of either effect sizes or selected power values that you want to look at
      - For every effect size, get a power
      - For every power, what is the minimal effect size that you could detect


- Have code that can generate data for this case:
  - [x] Conditions that are rolled out
      - [x] true / clean stepped wedge (baseline vs conversion to intervention)
      - everyone is measured at all times
      - [x] use a condition matrix as input
  - [x] 4 levels (cohort - where condition is assigned, clinic, patient, time)
      - specify numbers in nesting structure
      - [x] only things that change across time are patients (whatever their outcome variable is)
          - look at "reach" as the outcome variable for patients: what's the proportion of people screened positive, who eventually get treatment. Among positive screens, who actually gets into treatment
          - [x] generate a changing number of patients each time (poisson variable)
  - [x] With unbalanced / uneven group sizes (that the user can specify)


- Have a model together that generates random intercepts
- Generate a flow diagram or some pseudocode

- Assigment matrix
  Columns are timepoints
  Rows are the units who will be assigned (e)

- Specify the levels of the hierarchy
- which level of it is used for assignment (hospitals, doctors, etc)
- How many of the next lowest level are within each of the assigned level
- The level below that (e.g., patients) will generally not be the same at each timepoint (this is the denominator for something like reach)
    - So these get newly generated at each timepoint
    - So we'll need to specify the distribution with which they are generated
- For each condition over time, what does the numerator / improvement in the outcome look like? These are fixed effects (then we can add the distribution of errors here too)
- Somehow we need to include ICCs (patients within doctors, doctors within hospitals) - consider taking logs of the proportions to do things on a log scale

- Specifying what the fixed effects are and what their parameters are and how they work on the level of where the assignment occurs (everything else above and below should be random)

- Specify information about number of replications for the generated data. Is the model for analysis the same as the generating model?

- Using a standard binomial proportion variance (pq/n), we want that to be able to vary across situations. For example, MPLUS there is a random slope where the mean is 0, but the regression variance is some fixed quantity.
 
```{r}
library(tidyverse)

site_data_df <- site_data_ex # registered example site data to package

site_data_df <- site_data_df |> 
  rename(school = hospital, num_teachers = num_pcps)


sim_samples_df <- stepped_wedge_ex |> 
  pivot_schedule_longer(t1:t9) |> # added condition time
  join_unit_info(site_data_df, by = "cohort", uncount_unit = num_teachers, .id = "teacher") |> 
  initialize_replicates(n = 10)
```

# 2024-06-28

We have two items to work on now:
  1. How do we allow the parameters to vary (e.g., effect sizes, ICCs). Maybe we constrain users to have to change those manually?
  2. How do we calculate output-related content? We want to be able to calculate not just our own p-values, but also our own critical values (e.g., by computing the 95th percentiles of the observed test statistic across our empirical simulated distribution of samples).
       - For now, let's assume we're doing one analysis at a time and doing one replication. Let's keep track of a way to save each model's results so we can compare results from different models on the same dataset.
       - Allow ourselves to compare the results of LRT tests.
       - Calculate variance of random effect and do comparison where you fix that variance to zero and run a model comparing the two (with an LRT)
       - Overall, we want to have a convenient *labelling* of these objects to keep track of.
       
       
       
  3. Also, think of a way to capitalize on starting values to speed up lmer (but do this last)
  

```{r}
sim_samples_df <- sim_samples_df |> 
  add_fixed_effect(condition = ifelse(condition == "intervention", 2 + .15*condition_time, 0)) |> 
  group_by(sim_sample, school, teacher) |> 
  add_random_intercept(variance = 1) |> 
  add_error(variance = 1) |> 
  mutate(.response = .10*time + .condition + .i_sim_sample_school_teacher + .error)

sim_samples_df
```


```{r}
library(lmerTest)

results_df <- sim_samples_df |>
  group_by(sim_sample) |>
  fit_models(lmer, formula = .response ~ time + .condition + (1 | school/teacher))

results_df |> 
  tidy_models()
```
```{r}
library(future)

plan(multisession)


results_df <- sim_samples_df |>
  group_by(sim_sample) |>
  fit_models(lmer, formula = .response ~ time + .condition + (1 | school/teacher)) |> 

results_df |> 
  tidy_models()
```


```{r}
library(future)

plan(multisession)

results_df <- progressr::with_progress({
  sim_samples_df |> 
  group_by(sim_sample) |> 
  fit_models_progress(.response ~ time + .condition + (1 | school/teacher))
})
```



```{r}
library(lmerTest)

# cohort/hospital/pcp
# pcp/hospital/cohort

# fit <- lmer(.response ~ .condition_effect + (1 | cohort/hospital/pcp), data = df)
# summary(fit)

fit_df <- sim_samples_df |> 
  group_by(sim_sample) |> 
  nest() |> 
  mutate(
    fit = map(
      .x = data, 
      .f = ~ lmer(
        formula = .response ~ time + .condition + (1 | school/teacher), 
        data = .x)),
    results = map(fit, broom.mixed::tidy))

```

```{r}
pop_coefs <- list(
  time = .10,
  .condition = 2
)

fit_df |> 
  summarize_models(pop_coefs = pop_coefs)
```




























